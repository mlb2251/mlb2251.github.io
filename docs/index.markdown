---
# Feel free to add content and custom Front Matter to this file.
# To modify the layout, see https://jekyllrb.com/docs/themes/#overriding-theme-defaults
layout: single
classes: wide

---
Updated: Dec 2020

I'm a PhD candidate in CSAIL at MIT co-advised by [Armando Solar-Lezama](https://people.csail.mit.edu/asolar/) in EECS and [Josh Tenenbaum](http://cocosci.mit.edu/josh) in BCS. My research is in program synthesis and artificial intelligence.

See our latest work, presented at the NeurIPS 2020 Workshop on Computer-Assisted Programming Workshop and submitted to ICLR 2021: [Representing Partial Programs with Blended Abstract Semantics](https://arxiv.org/pdf/2012.12964).

A list of publications can be found on my [Google Scholar](https://scholar.google.com/citations?user=ghdbIsoAAAAJ).


attach resume

I'm interested in combining program synthesis techniques from the programming languages community with deep learning methods.

compression

put the programs in the brain

At a high level, I'm interested in developing AI systems with more features of human intelligence. 

Program induction is a promising approach to this.

I'm particularly interested in developing and applying methods in program synthesis to this end. I'd like to combine techniques from the programming languages community with deep learning methods 

I'm interested in developing 


Some of the areas I'm interested in include
- neurosj
- Concept/library learning and compression, as in [DreamCoder](https://arxiv.org/abs/2006.08381)
- Predicate learning, as in [BUSTLE](https://arxiv.org/abs/2007.14381) and [Property Signatures](https://arxiv.org/abs/2002.09030)
- Applying program synthesis to developing interpretable scientific models, as in our [NSF Expeditions project](http://www.neurosymbolic.org/)


- Developing the fundamental methods of neurosymbolic program synthesis (interpretable, generalizable, low data)
- Designing learning systems that are modeled after features of human intelligence
- I'm interested in "neurosymbolic" systems - AI systems that combine techniques from the programming languages community with those of the deep learning community.



neurosymbolic
machine learning

In 2020 I graduated from Columbia University with a BA in Computer science and a BA in Chemistry. At Columbia, I worked with Professor Angelo Cacciuto on chemical simulations of self-assembling colloids and authored two publications:
- Das, S., Lee Bowers, M., Bakker, C., & Cacciuto, A. (2019). Active sculpting of colloidal crystals. *The Journal of Chemical Physics*, 150 (13), 134505.
- Mallory, S., Lee Bowers, M., & Cacciuto, A. (2020). Universal reshaping of arrested colloidal gels via active doping. *The Journal of Chemical Physics*, 153, 084901.

In the summer of 2019, I worked in the [Learning Matter Group](http://gomezbombarelli.mit.edu/) at MIT under Professor Rafael Gomez-Bombarelli, where I applied graph neural network methods to molecular property prediction. I'm interested in combining these methods with program synthesis techniques in the future.


I also wrote [Coral](https://github.com/jacobaustin123/Coral), a gradually-typed Python compiler which runs type inference and generates fast equivalent LLVM-IR code whenever possible.


